<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>KF Basics - Part 2 &mdash; PythonRobotics  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/plot_directive.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="How To Contribute" href="../../how_to_contribute.html" />
    <link rel="prev" title="KF Basics - Part I" href="Kalmanfilter_basics.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

            <a href="../../index.html" class="icon icon-home"> PythonRobotics
            <img src="../../_static/icon.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9612347954373886"
     crossorigin="anonymous"></script>
<!-- PythonRoboticsDoc -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-9612347954373886"
     data-ad-slot="1579532132"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../localization/localization.html">Localization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mapping/mapping.html">Mapping</a></li>
<li class="toctree-l1"><a class="reference internal" href="../slam/slam.html">SLAM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../path_planning/path_planning.html">Path Planning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../path_tracking/path_tracking.html">Path Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../arm_navigation/arm_navigation.html">Arm Navigation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../aerial_navigation/aerial_navigation.html">Aerial Navigation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bipedal/bipedal.html">Bipedal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../control/control.html">Control</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/utils.html">Utilities</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="appendix.html">Appendix</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Kalmanfilter_basics.html">KF Basics - Part I</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">KF Basics - Part 2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#probabilistic-generative-laws">Probabilistic Generative Laws</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#st-law">1st Law:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nd-law">2nd Law:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#conditional-dependence-and-independence-example">Conditional dependence and independence example:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bayes-rule">Bayes Rule:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bayes-filter-algorithm">Bayes Filter Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bayes-filter-localization-example">Bayes filter localization example:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bayes-and-kalman-filter-structure">Bayes and Kalman filter structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kalman-gain">Kalman Gain</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kalman-filter-univariate-and-multivariate">Kalman Filter - Univariate and Multivariate</a></li>
<li class="toctree-l3"><a class="reference internal" href="#references">References:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../how_to_contribute.html">How To Contribute</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">PythonRobotics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="appendix.html">Appendix</a> &raquo;</li>
      <li>KF Basics - Part 2</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/AtsushiSakai/PythonRobotics/blob/master/docs/modules/appendix/Kalmanfilter_basics_2_main.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="kf-basics-part-2">
<h1>KF Basics - Part 2<a class="headerlink" href="#kf-basics-part-2" title="Permalink to this headline"></a></h1>
<section id="probabilistic-generative-laws">
<h2>Probabilistic Generative Laws<a class="headerlink" href="#probabilistic-generative-laws" title="Permalink to this headline"></a></h2>
<section id="st-law">
<h3>1st Law:<a class="headerlink" href="#st-law" title="Permalink to this headline"></a></h3>
<p>The belief representing the state <span class="math notranslate nohighlight">\(x_{t}\)</span>, is conditioned on all
past states, measurements and controls. This can be shown mathematically
by the conditional probability shown below:</p>
<div class="math notranslate nohighlight">
\[p(x_{t} | x_{0:t-1},z_{1:t-1},u_{1:t})\]</div>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(z_{t}\)</span> represents the <strong>measurement</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(u_{t}\)</span> the <strong>motion command</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(x_{t}\)</span> the <strong>state</strong> (can be the position, velocity, etc) of
the robot or its environment at time t.</p></li>
</ol>
<p>‘If we know the state <span class="math notranslate nohighlight">\(x_{t-1}\)</span> and <span class="math notranslate nohighlight">\(u_{t}\)</span>, then knowing
the states <span class="math notranslate nohighlight">\(x_{0:t-2}\)</span>, <span class="math notranslate nohighlight">\(z_{1:t-1}\)</span> becomes immaterial
through the property of <strong>conditional independence</strong>’. The state
<span class="math notranslate nohighlight">\(x_{t-1}\)</span> introduces a conditional independence between
<span class="math notranslate nohighlight">\(x_{t}\)</span> and <span class="math notranslate nohighlight">\(z_{1:t-1}\)</span>, <span class="math notranslate nohighlight">\(u_{1:t-1}\)</span></p>
<p>Therefore the law now holds as:</p>
<div class="math notranslate nohighlight">
\[p(x_{t} | x_{0:t-1},z_{1:t-1},u_{1:t})=p(x_{t} | x_{t-1},u_{t})\]</div>
</section>
<section id="nd-law">
<h3>2nd Law:<a class="headerlink" href="#nd-law" title="Permalink to this headline"></a></h3>
<p>If <span class="math notranslate nohighlight">\(x_{t}\)</span> is complete, then:</p>
<div class="math notranslate nohighlight">
\[p(z_{t} | x-_{0:t},z_{1:t-1},u_{1:t})=p(z_{t} | x_{t})\]</div>
<p><span class="math notranslate nohighlight">\(x_{t}\)</span> is <strong>complete</strong> means that the past states, controls or
measurements carry no additional information to predict future.</p>
<p><span class="math notranslate nohighlight">\(x_{0:t-1}\)</span>, <span class="math notranslate nohighlight">\(z_{1:t-1}\)</span> and <span class="math notranslate nohighlight">\(u_{1:t}\)</span> are
<strong>conditionally independent</strong> of <span class="math notranslate nohighlight">\(z_{t}\)</span> given <span class="math notranslate nohighlight">\(x_{t}\)</span> of
complete.</p>
<p>The filter works in two parts:</p>
<p><span class="math notranslate nohighlight">\(p(x_{t} | x_{t-1},u_{t})\)</span> -&gt; <strong>State Transition Probability</strong></p>
<p><span class="math notranslate nohighlight">\(p(z_{t} | x_{t})\)</span> -&gt; <strong>Measurement Probability</strong></p>
</section>
</section>
<section id="conditional-dependence-and-independence-example">
<h2>Conditional dependence and independence example:<a class="headerlink" href="#conditional-dependence-and-independence-example" title="Permalink to this headline"></a></h2>
<p><span class="math notranslate nohighlight">\(\bullet\)</span><strong>Independent but conditionally dependent</strong></p>
<p>Let’s say you flip two fair coins</p>
<p>A - Your first coin flip is heads</p>
<p>B - Your second coin flip is heads</p>
<p>C - Your first two flips were the same</p>
<p>A and B here are independent. However, A and B are conditionally
dependent given C, since if you know C then your first coin flip will
inform the other one.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span><strong>Dependent but conditionally independent</strong></p>
<p>A box contains two coins: a regular coin and one fake two-headed coin
((P(H)=1). I choose a coin at random and toss it twice. Define the
following events.</p>
<p>A= First coin toss results in an H.</p>
<p>B= Second coin toss results in an H.</p>
<p>C= Coin 1 (regular) has been selected.</p>
<p>If we know A has occurred (i.e., the first coin toss has resulted in
heads), we would guess that it is more likely that we have chosen Coin 2
than Coin 1. This in turn increases the conditional probability that B
occurs. This suggests that A and B are not independent. On the other
hand, given C (Coin 1 is selected), A and B are independent.</p>
</section>
<section id="bayes-rule">
<h2>Bayes Rule:<a class="headerlink" href="#bayes-rule" title="Permalink to this headline"></a></h2>
<p>Posterior =</p>
<div class="math notranslate nohighlight">
\[\frac{Likelihood*Prior}{Marginal}\]</div>
<p>Here,</p>
<p><strong>Posterior</strong> = Probability of an event occurring based on certain
evidence.</p>
<p><strong>Likelihood</strong> = How probable is the evidence given the event.</p>
<p><strong>Prior</strong> = Probability of the just the event occurring without having
any evidence.</p>
<p><strong>Marginal</strong> = Probability of the evidence given all the instances of
events possible.</p>
<p>Example:</p>
<p>1% of women have breast cancer (and therefore 99% do not). 80% of
mammograms detect breast cancer when it is there (and therefore 20% miss
it). 9.6% of mammograms detect breast cancer when its not there (and
therefore 90.4% correctly return a negative result).</p>
<p>We can turn the process above into an equation, which is Bayes Theorem.
Here is the equation:</p>
<p><span class="math notranslate nohighlight">\(\displaystyle{\Pr(\mathrm{A}|\mathrm{X}) = \frac{\Pr(\mathrm{X}|\mathrm{A})\Pr(\mathrm{A})}{\Pr(\mathrm{X|A})\Pr(\mathrm{A})+ \Pr(\mathrm{X | not \ A})\Pr(\mathrm{not \ A})}}\)</span></p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span>Pr(A|X) = Chance of having cancer (A) given a positive
test (X). This is what we want to know: How likely is it to have cancer
with a positive result? In our case it was 7.8%.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span>Pr(X|A) = Chance of a positive test (X) given that you
had cancer (A). This is the chance of a true positive, 80% in our case.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span>Pr(A) = Chance of having cancer (1%).</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span>Pr(not A) = Chance of not having cancer (99%).</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span>Pr(X|not A) = Chance of a positive test (X) given that
you didn’t have cancer (~A). This is a false positive, 9.6% in our case.</p>
</section>
<section id="bayes-filter-algorithm">
<h2>Bayes Filter Algorithm<a class="headerlink" href="#bayes-filter-algorithm" title="Permalink to this headline"></a></h2>
<p>The basic filter algorithm is:</p>
<p>for all <span class="math notranslate nohighlight">\(x_{t}\)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\overline{bel}(x_t) = \int p(x_t | u_t, x_{t-1}) bel(x_{t-1})dx\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(bel(x_t) = \eta p(z_t | x_t) \overline{bel}(x_t)\)</span></p></li>
</ol>
<p>end.</p>
<p><span class="math notranslate nohighlight">\(\rightarrow\)</span>The first step in filter is to calculate the prior
for the next step that uses the bayes theorem. This is the
<strong>Prediction</strong> step. The belief, <span class="math notranslate nohighlight">\(\overline{bel}(x_t)\)</span>, is
<strong>before</strong> incorporating measurement(<span class="math notranslate nohighlight">\(z_{t}\)</span>) at time t=t. This
is the step where the motion occurs and information is lost because the
means and covariances of the gaussians are added. The RHS of the
equation incorporates the law of total probability for prior
calculation.</p>
<p><span class="math notranslate nohighlight">\(\rightarrow\)</span> This is the <strong>Correction</strong> or update step that
calculates the belief of the robot <strong>after</strong> taking into account the
measurement(<span class="math notranslate nohighlight">\(z_{t}\)</span>) at time t=t. This is where we incorporate
the sensor information for the whereabouts of the robot. We gain
information here as the gaussians get multiplied here. (Multiplication
of gaussian values allows the resultant to lie in between these numbers
and the resultant covariance is smaller.</p>
</section>
<section id="bayes-filter-localization-example">
<h2>Bayes filter localization example:<a class="headerlink" href="#bayes-filter-localization-example" title="Permalink to this headline"></a></h2>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;bayes_filter.png&quot;</span><span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../_images/Kalmanfilter_basics_2_5_0.png"><img alt="../../_images/Kalmanfilter_basics_2_5_0.png" src="../../_images/Kalmanfilter_basics_2_5_0.png" style="width: 400px;" /></a>
<p>Given - A robot with a sensor to detect doorways along a hallway. Also,
the robot knows how the hallway looks like but doesn’t know where it is
in the map.</p>
<ol class="arabic simple">
<li><p>Initially(first scenario), it doesn’t know where it is with respect
to the map and hence the belief assigns equal probability to each
location in the map.</p></li>
<li><p>The first sensor reading is incorporated and it shows the presence of
a door. Now the robot knows how the map looks like but cannot
localize yet as map has 3 doors present. Therefore it assigns equal
probability to each door present.</p></li>
<li><p>The robot now moves forward. This is the prediction step and the
motion causes the robot to lose some of the information and hence the
variance of the gaussians increase (diagram 4.). The final belief is
<strong>convolution</strong> of posterior from previous step and the current state
after motion. Also, the means shift on the right due to the motion.</p></li>
<li><p>Again, incorporating the measurement, the sensor senses a door and
this time too the possibility of door is equal for the three door.
This is where the filter’s magic kicks in. For the final belief
(diagram 5.), the posterior calculated after sensing is mixed or
<strong>convolution</strong> of previous posterior and measurement. It improves
the robot’s belief at location near to the second door. The variance
<strong>decreases</strong> and <strong>peaks</strong>.</p></li>
<li><p>Finally after series of iterations of motion and correction, the
robot is able to localize itself with respect to the
environment.(diagram 6.)</p></li>
</ol>
<p>Do note that the robot knows the map but doesn’t know where exactly it
is on the map.</p>
</section>
<section id="bayes-and-kalman-filter-structure">
<h2>Bayes and Kalman filter structure<a class="headerlink" href="#bayes-and-kalman-filter-structure" title="Permalink to this headline"></a></h2>
<p>The basic structure and the concept remains the same as bayes filter for
Kalman. The only key difference is the mathematical representation of
Kalman filter. The Kalman filter is nothing but a bayesian filter that
uses Gaussians.</p>
<p>For a bayes filter to be a Kalman filter, <strong>each term of belief is now a
gaussian</strong>, unlike histograms. The basic formulation for the <strong>bayes
filter</strong> algorithm is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\bar {\mathbf x} &amp;= \mathbf x \ast f_{\mathbf x}(\bullet)\, \, &amp;\text{Prediction} \\
\mathbf x &amp;= \mathcal L \cdot \bar{\mathbf x}\, \, &amp;\text{Correction}
\end{aligned}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\bar{\mathbf x}\)</span> is the <em>prior</em></p>
<p><span class="math notranslate nohighlight">\(\mathcal L\)</span> is the <em>likelihood</em> of a measurement given the prior
<span class="math notranslate nohighlight">\(\bar{\mathbf x}\)</span></p>
<p><span class="math notranslate nohighlight">\(f_{\mathbf x}(\bullet)\)</span> is the <em>process model</em> or the gaussian
term that helps predict the next state like velocity to track position
or acceleration.</p>
<p><span class="math notranslate nohighlight">\(\ast\)</span> denotes <em>convolution</em>.</p>
</section>
<section id="kalman-gain">
<h2>Kalman Gain<a class="headerlink" href="#kalman-gain" title="Permalink to this headline"></a></h2>
<div class="math notranslate nohighlight">
\[x = (\mathcal L \bar x)\]</div>
<p>Where x is posterior and <span class="math notranslate nohighlight">\(\mathcal L\)</span> and <span class="math notranslate nohighlight">\(\bar x\)</span> are
gaussians.</p>
<p>Therefore the mean of the posterior is given by:</p>
<div class="math notranslate nohighlight">
\[\mu=\frac{\bar\sigma^2\, \mu_z + \sigma_z^2 \, \bar\mu} {\bar\sigma^2 + \sigma_z^2}\]</div>
<div class="math notranslate nohighlight">
\[\mu = \left( \frac{\bar\sigma^2}{\bar\sigma^2 + \sigma_z^2}\right) \mu_z + \left(\frac{\sigma_z^2}{\bar\sigma^2 + \sigma_z^2}\right)\bar\mu\]</div>
<p>In this form it is easy to see that we are scaling the measurement and
the prior by weights:</p>
<div class="math notranslate nohighlight">
\[\mu = W_1 \mu_z + W_2 \bar\mu\]</div>
<p>The weights sum to one because the denominator is a normalization term.
We introduce a new term, <span class="math notranslate nohighlight">\(K=W_1\)</span>, giving us:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mu &amp;= K \mu_z + (1-K) \bar\mu\\
&amp;= \bar\mu + K(\mu_z - \bar\mu)
\end{aligned}\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[K = \frac {\bar\sigma^2}{\bar\sigma^2 + \sigma_z^2}\]</div>
<p>The variance in terms of the Kalman gain:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\sigma^2 &amp;= \frac{\bar\sigma^2 \sigma_z^2 } {\bar\sigma^2 + \sigma_z^2} \\
&amp;= K\sigma_z^2 \\
&amp;= (1-K)\bar\sigma^2
\end{aligned}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(K\)</span> is the <em>Kalman gain</em>. It’s the crux of the Kalman filter. It
is a scaling term that chooses a value partway between <span class="math notranslate nohighlight">\(\mu_z\)</span> and
<span class="math notranslate nohighlight">\(\bar\mu\)</span>.</p>
</section>
<section id="kalman-filter-univariate-and-multivariate">
<h2>Kalman Filter - Univariate and Multivariate<a class="headerlink" href="#kalman-filter-univariate-and-multivariate" title="Permalink to this headline"></a></h2>
<p><strong>Prediction</strong></p>
<p><span class="math notranslate nohighlight">\(\begin{array}{|l|l|l|} \hline \text{Univariate} &amp; \text{Univariate} &amp; \text{Multivariate}\\ &amp; \text{(Kalman form)} &amp; \\ \hline \bar \mu = \mu + \mu_{f_x} &amp; \bar x = x + dx &amp; \bar{\mathbf x} = \mathbf{Fx} + \mathbf{Bu}\\ \bar\sigma^2 = \sigma_x^2 + \sigma_{f_x}^2 &amp; \bar P = P + Q &amp; \bar{\mathbf P} = \mathbf{FPF}^\mathsf T + \mathbf Q \\ \hline \end{array}\)</span></p>
<p><span class="math notranslate nohighlight">\(\mathbf x,\, \mathbf P\)</span> are the state mean and covariance. They
correspond to <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\mathbf F\)</span> is the <em>state transition function</em>. When multiplied by
<span class="math notranslate nohighlight">\(\bf x\)</span> it computes the prior.</p>
<p><span class="math notranslate nohighlight">\(\mathbf Q\)</span> is the process covariance. It corresponds to
<span class="math notranslate nohighlight">\(\sigma^2_{f_x}\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\mathbf B\)</span> and <span class="math notranslate nohighlight">\(\mathbf u\)</span> are model control inputs to the
system.</p>
<p><strong>Correction</strong></p>
<p><span class="math notranslate nohighlight">\(\begin{array}{|l|l|l|} \hline \text{Univariate} &amp; \text{Univariate} &amp; \text{Multivariate}\\ &amp; \text{(Kalman form)} &amp; \\ \hline &amp; y = z - \bar x &amp; \mathbf y = \mathbf z - \mathbf{H\bar x} \\ &amp; K = \frac{\bar P}{\bar P+R}&amp; \mathbf K = \mathbf{\bar{P}H}^\mathsf T (\mathbf{H\bar{P}H}^\mathsf T + \mathbf R)^{-1} \\ \mu=\frac{\bar\sigma^2\, \mu_z + \sigma_z^2 \, \bar\mu} {\bar\sigma^2 + \sigma_z^2} &amp; x = \bar x + Ky &amp; \mathbf x = \bar{\mathbf x} + \mathbf{Ky} \\ \sigma^2 = \frac{\sigma_1^2\sigma_2^2}{\sigma_1^2+\sigma_2^2} &amp; P = (1-K)\bar P &amp; \mathbf P = (\mathbf I - \mathbf{KH})\mathbf{\bar{P}} \\ \hline \end{array}\)</span></p>
<p><span class="math notranslate nohighlight">\(\mathbf H\)</span> is the measurement function.</p>
<p><span class="math notranslate nohighlight">\(\mathbf z,\, \mathbf R\)</span> are the measurement mean and noise
covariance. They correspond to <span class="math notranslate nohighlight">\(z\)</span> and <span class="math notranslate nohighlight">\(\sigma_z^2\)</span> in the
univariate filter. <span class="math notranslate nohighlight">\(\mathbf y\)</span> and <span class="math notranslate nohighlight">\(\mathbf K\)</span> are the
residual and Kalman gain.</p>
<p>The details will be different than the univariate filter because these
are vectors and matrices, but the concepts are exactly the same:</p>
<ul class="simple">
<li><p>Use a Gaussian to represent our estimate of the state and error</p></li>
<li><p>Use a Gaussian to represent the measurement and its error</p></li>
<li><p>Use a Gaussian to represent the process model</p></li>
<li><p>Use the process model to predict the next state (the prior)</p></li>
<li><p>Form an estimate part way between the measurement and the prior</p></li>
</ul>
</section>
<section id="references">
<h2>References:<a class="headerlink" href="#references" title="Permalink to this headline"></a></h2>
<ol class="arabic simple">
<li><p>Roger Labbe’s
<a class="reference external" href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python">repo</a>
on Kalman Filters. (Majority of text in the notes are from this)</p></li>
<li><p>Probabilistic Robotics by Sebastian Thrun, Wolfram Burgard and Dieter
Fox, MIT Press.</p></li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Kalmanfilter_basics.html" class="btn btn-neutral float-left" title="KF Basics - Part I" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../how_to_contribute.html" class="btn btn-neutral float-right" title="How To Contribute" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018-2021, Atsushi Sakai.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>